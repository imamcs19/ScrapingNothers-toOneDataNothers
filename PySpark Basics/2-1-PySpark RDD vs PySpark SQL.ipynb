{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/header1.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: left;\">ToT: PySpark Basics to Inter./Adv. in Â± 15 Minutes</h1>\n",
    "By: Imam Cholissodin | Dosen Fakultas Ilmu Komputer (FILKOM), Universitas Brawijaya (UB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-koding-dasar\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://46a00a9a27eb:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-koding-dasar</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7d1021f748>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://46a00a9a27eb:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-koding-dasar</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=pyspark-koding-dasar>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contoh Koding Spark dengan Resilient Distributed Datasets (RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](ilustrasi-rdd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semuanya berjalan di Memory, sehingga bisa lebih cepat dari pada Map Reduce dari Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cek banyak node\n",
    "sc._jsc.sc().getExecutorMemoryStatus().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# Koding Spark RDD | Map\n",
    "def add1(x): \n",
    "    return x+5\n",
    "\n",
    "raw_data = sc.parallelize([1,2,3])\n",
    "rdd = raw_data.map(lambda x: add1(x))\n",
    "#print(rdd.take(3))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "# Koding Spark RDD | Filter\n",
    "def isGanjil(x): \n",
    "    return x%2==1\n",
    "\n",
    "raw_data = sc.parallelize(range(1,6))\n",
    "rdd = raw_data.filter(lambda x: isGanjil(x))\n",
    "#print(rdd.take(3))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Koding Spark RDD | Reduce\n",
    "def jumlahkan(x,y): \n",
    "    return x+y\n",
    "\n",
    "raw_data = sc.parallelize(range(1,6))\n",
    "#rdd = raw_data.reduce(lambda x,y:x+y)\n",
    "\n",
    "# 1 + 2 + 3 + 4 + 5 = 15\n",
    "rdd = raw_data.reduce(lambda x,y:jumlahkan(x,y))\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Koding Spark RDD | Lambda Spark (tanpa memberikan nama fungsi)\n",
    "raw_data = sc.parallelize(range(1,6))\n",
    "rdd = raw_data.reduce(lambda x,y:x+y)\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# python standar\n",
    "(lambda x:2*x)(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dalam dalam bentuk user define function (UDF), misal dalam variabel f\n",
    "f = lambda x:2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "# Koding Spark RDD | Mengalikan setiap elemen raw_data dengan 2\n",
    "raw_data = sc.parallelize([1,2,3])\n",
    "rdd=raw_data.map(lambda x:2*x)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "# Koding Spark RDD | Mengalikan setiap elemen raw_data dengan 2\n",
    "raw_data = sc.parallelize([1,2,3])\n",
    "rdd=raw_data.map(f)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "# Koding Spark RDD | Mengambil elemen awal dari tiap pasang raw_data\n",
    "raw_data = sc.parallelize([(1,2),(3,4),(5,6)])\n",
    "rdd=raw_data.map(lambda x:x[0])\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Koding Spark RDD | Menjumlahkan semua elemen pada raw_data\n",
    "raw_data = sc.parallelize([1,2,3])\n",
    "rdd = raw_data.reduce(lambda x,y:x+y)\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# Koding Spark RDD | Menjumlahkan hanya elemen pertama tiap pasangan raw_data\n",
    "raw_data = sc.parallelize([(1,2),(3,4),(5,6)])\n",
    "rdd=raw_data.map(lambda x:x[0]).reduce(lambda x,y:x+y)\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tambahan 1 : Koding dengan Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    57  100    57    0     0    553      0 --:--:-- --:--:-- --:--:--   553\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/imamcs19/ScrapingNothers-toOneDataNothers/master/nilai-uas.csv > nilai-uas-mhs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan data awal, yaitu mahasiswa dan nilai uas-nya:\n",
      "[('Mhs 1', 95), ('Mhs 2', 70), ('Mhs 3', 85), ('Mhs 4', 100)]\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "# data = sc.parallelize([(\"Mhs 1\", 95), (\"Mhs 2\", 70), (\"Mhs 3\", 85), (\"Mhs 4\", 100)])\n",
    "data = sc.textFile(\"nilai-uas-mhs.csv\") \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .zipWithIndex().filter(lambda baris: baris[1] > 0)\\\n",
    "    .map(lambda x:x[0])\\\n",
    "    .map(lambda x:(x[0],int(x[1])))\n",
    "\n",
    "# menampilkan data awal, yaitu mahasiswa dan nilai uas-nya\n",
    "print(\"Menampilkan data awal, yaitu mahasiswa dan nilai uas-nya:\")\n",
    "print(data.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan hanya nama mahasiswa:\n",
      "['Mhs 1', 'Mhs 2', 'Mhs 3', 'Mhs 4']\n"
     ]
    }
   ],
   "source": [
    "print(\"Menampilkan hanya nama mahasiswa:\")\n",
    "hasil1=data.map(lambda x: x[0]).collect()\n",
    "print(hasil1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan hanya nilai uas semua mahasiswa:\n",
      "[95, 70, 85, 100]\n"
     ]
    }
   ],
   "source": [
    "print(\"Menampilkan hanya nilai uas semua mahasiswa:\")\n",
    "hasil2=data.map(lambda t: t[1]).collect()\n",
    "print(hasil2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan total nilai uas semua mahasiswa-nya:\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "print(\"Menampilkan total nilai uas semua mahasiswa-nya:\")\n",
    "raw_data = sc.parallelize(hasil2)\n",
    "hasil3 = raw_data.reduce(lambda x,y:x+y)\n",
    "print(hasil3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan total nilai uas semua mahasiswa-nya:\n",
      "[(1, 350)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Menampilkan total nilai uas semua mahasiswa-nya:\")\n",
    "hasil31=data.map(lambda t: (1,int(t[1]))).reduceByKey(lambda x,y: x+y).collect()\n",
    "print(hasil31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan nilai minimum dari semua mahasiswa-nya:\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(\"Menampilkan nilai minimum dari semua mahasiswa-nya:\")\n",
    "raw_data = sc.parallelize(hasil2)\n",
    "hasil4 = raw_data.reduce(lambda x,y:min(x,y))\n",
    "print(hasil4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan nilai minimum dari semua mahasiswa-nya:\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(\"Menampilkan nilai maksimum dari semua mahasiswa-nya:\")\n",
    "raw_data = sc.parallelize(hasil2)\n",
    "hasil5 = raw_data.reduce(lambda x,y:max(x,y))\n",
    "print(hasil5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan banyak mahasiswa:\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(\"Menampilkan banyak mahasiswa:\")\n",
    "hasil6 = data.map(lambda t: 1).reduce(lambda x,y:x+y)\n",
    "print(hasil6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan nilai rata-rata dari semua nilai uas mahasiswa:\n",
      "87.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Menampilkan nilai rata-rata dari semua nilai uas mahasiswa:\")\n",
    "hasil7=data.map(lambda x:x[1]).reduce(lambda x,y:x+y) / data.map(lambda x:1).reduce(lambda x,y:x+y)\n",
    "print(hasil7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tambahan 2 : Koding dengan Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan data awal, yaitu mahasiswa dan nilai uas-nya:\n",
      "+-----+---------+\n",
      "| Nama|Nilai UAS|\n",
      "+-----+---------+\n",
      "|Mhs 1|       95|\n",
      "|Mhs 2|       70|\n",
      "|Mhs 3|       85|\n",
      "|Mhs 4|      100|\n",
      "+-----+---------+\n",
      "\n",
      "\n",
      "\n",
      "Menampilkan nilai uas yang paling minimal cara ke-1:\n",
      "+-------------+\n",
      "|min Nilai UAS|\n",
      "+-------------+\n",
      "|           70|\n",
      "+-------------+\n",
      "\n",
      "\n",
      "\n",
      "Menampilkan nilai uas yang paling minimal cara ke-2:\n",
      "+--------------+\n",
      "|min(Nilai UAS)|\n",
      "+--------------+\n",
      "|            70|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "#from pyspark.sql import functions as F    dimana min => F.min, max => F.max\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "# cara convert rdd ke df\n",
    "# rdd2df=data.toDF([\"Nama\",\"Nilai UAS\"])\n",
    "\n",
    "# atau\n",
    "rdd2df = spark.read.csv(path=\"nilai-uas-mhs.csv\", sep=\",\", header=True)\n",
    "\n",
    "# convert kolom \"Nilai UAS\" menjadi integer\n",
    "rdd2df = rdd2df.withColumn(\"Nilai UAS\", col(\"Nilai UAS\").cast('int'))\n",
    "\n",
    "# df = spark.createDataFrame([(\"Mhs 1\", 95), (\"Mhs 2\", 70), (\"Mhs 3\", 85), (\"Mhs 4\", 100)], [\"Nama\", \"Nilai UAS\"])\n",
    "\n",
    "# menampilkan data awal, yaitu mahasiswa dan nilai uas-nya\n",
    "print(\"Menampilkan data awal, yaitu mahasiswa dan nilai uas-nya:\")\n",
    "rdd2df.show()\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Menampilkan nilai uas yang paling minimal cara ke-1:\")\n",
    "minNilaiUAS = rdd2df \\\n",
    "    .groupBy() \\\n",
    "    .min('Nilai UAS') \\\n",
    "    .select(col(\"min(Nilai UAS)\").alias(\"min Nilai UAS\"))\n",
    "    \n",
    "minNilaiUAS.show()\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Menampilkan nilai uas yang paling minimal cara ke-2:\")\n",
    "hasil=df.groupBy().agg(min(col('Nilai UAS')))\n",
    "hasil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data nilai uas mahasiswa:\n",
      "+-----+-------------+---------+\n",
      "| Nama|Jenis Kelamin|Nilai UAS|\n",
      "+-----+-------------+---------+\n",
      "|Mhs 1|            P|       95|\n",
      "|Mhs 2|            L|       70|\n",
      "|Mhs 3|            P|       85|\n",
      "|Mhs 4|            L|      100|\n",
      "+-----+-------------+---------+\n",
      "\n",
      "\n",
      "\n",
      "Menampilkan rata-rata nilai uas berdasarkan JK:\n",
      "+-------------+--------------+\n",
      "|Jenis Kelamin|avg(Nilai UAS)|\n",
      "+-------------+--------------+\n",
      "|            L|          85.0|\n",
      "|            P|          90.0|\n",
      "+-------------+--------------+\n",
      "\n",
      "\n",
      "Contoh pada Data Pajak Perusahaan:\n",
      "==============================\n",
      "Data Pajak Perusahaan:\n",
      "+---------------+----------------+------------------+\n",
      "|Nama Perusahaan|Jenis Perusahaan|Pajak (Rp x 1000K)|\n",
      "+---------------+----------------+------------------+\n",
      "|   Perusahaan 1|          Tipe B|              1000|\n",
      "|   Perusahaan 2|          Tipe C|               500|\n",
      "|   Perusahaan 3|          Tipe A|             10000|\n",
      "|   Perusahaan 4|          Tipe B|              1000|\n",
      "+---------------+----------------+------------------+\n",
      "\n",
      "\n",
      "\n",
      "Menampilkan jumlah pajak berdasarkan Jenis Perusahaan:\n",
      "+----------------+-----------------------+\n",
      "|Jenis Perusahaan|sum(Pajak (Rp x 1000K))|\n",
      "+----------------+-----------------------+\n",
      "|          Tipe A|                  10000|\n",
      "|          Tipe C|                    500|\n",
      "|          Tipe B|                   2000|\n",
      "+----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# data nilai uas mahasiswa\n",
    "df = spark.createDataFrame(\n",
    "[(\"Mhs 1\", \"P\", 95), (\"Mhs 2\", \"L\", 70), \n",
    "(\"Mhs 3\", \"P\", 85), (\"Mhs 4\", \"L\", 100)], \n",
    "[\"Nama\", \"Jenis Kelamin\", \"Nilai UAS\"])\n",
    "\n",
    "print(\"Data nilai uas mahasiswa:\")\n",
    "df.show()\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Menampilkan rata-rata nilai uas berdasarkan JK:\")\n",
    "df.groupBy('Jenis Kelamin').agg(mean(col('Nilai UAS'))).show()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref:\n",
    "# [1] Cholissodin, I., Riyandani, E., 2016, Analisis Big Data, Fakultas Ilmu Komputer, Universitas Brawijaya, Malang.\n",
    "# [2] Cholissodin, I. and Supianto, A. A. 2019. Enhancement Full Open Source Hadoop Distribution Universal Big Data Up Projects (UBig) From Education To Enterprise. 2019 International Conference on Sustainable Information Engineering and Technology (SIET), Lombok, Indonesia, 2019, pp. 90-93, doi: 10.1109/SIET48054.2019.8986040.\n",
    "# [3] Miao, K., Li, J., Hong, W. & Chen, M. 2020. A Microservice-Based Big Data Analysis Platform for Online Educational Applications. Annual Review of Anthropology, 2020, [ \"6929750\" ]. Available from: https://doi.org/10.1146/annurev.anthro.33.070203.144008\n",
    "# [4] Roy, S., et al. 2017. IoT, big data science & analytics, cloud computing and mobile app based hybrid system for smart agriculture. 2017 8th Annual Industrial Automation and Electromechanical Engineering Conference (IEMECON), Bangkok, 2017, pp. 303-304, doi: 10.1109/IEMECON.2017.8079610.\n",
    "# [5] Dabek, F. 2016. Leveraging Big Data to Provide a Web Service That Provides the Likelihood of Developing Psychological Conditions after a Concussion. 2016 IEEE International Conference on Mobile Services (MS), San Francisco, CA, 2016, pp. 160-165, doi: 10.1109/MobServ.2016.32.\n",
    "# [6] Andy A. 2016. Cloud Computing Part 5: SaaS (Software as a Service). Available at: https://andypi.co.uk/2016/05/23/cloud-computing-part-5-saas-software-as-a-service/ (Accessed on July 30, 2020).\n",
    "# [7] Putra, N. A., Putri, A. T., Prabowo, D. A., Surtiningsih, L., Arniantya, R., Cholissodin, I. 2017. Klasifikasi Sepeda Motor Berdasarkan Karakteristik Konsumen Dengan Metode K-Nearest Neighbour Pada Big Data Menggunakan Hadoop Single Node Cluster. Jurnal Teknologi Informasi dan Ilmu Komputer (JTIIK) FILKOM UB Vol. 4 No. 2, 81-86.\n",
    "# [8] Naveen, N. 2020. Hadoop MapReduce â Key Features & Highlights. Available at: https://intellipaat.com/blog/tutorial/big-data-and-hadoop-tutorial/hadoop-mapreduce-key-features-highlights/ (Accessed on March 8, 2020).\n",
    "# [9] stackchief. 2017. MapReduce Quick Explanation. Available at: https://www.stackchief.com/blog/MapReduce%20Quick%20Explanation (Accessed on March 8, 2020).\n",
    "# [10] .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='another_cell'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/thumbs-up.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
