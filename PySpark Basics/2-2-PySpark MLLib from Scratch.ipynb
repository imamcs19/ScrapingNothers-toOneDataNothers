{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/header1.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: left;\">ToT: PySpark Basics to Inter./Adv. in Â± 15 Minutes</h1>\n",
    "By: Imam Cholissodin | Dosen Fakultas Ilmu Komputer (FILKOM), Universitas Brawijaya (UB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Konfigurasi utk konek ke Spark cluster, menggunakan SparkSession object dengan parameter berikut:\n",
    "\n",
    "+ **appName:** set nama app yang akan ditampilkan pada [Spark Master Web UI](http://localhost:8080/);\n",
    "+ **master:** set Spark Master URL, yang sudah terkoneksi dengan Spark Workers;\n",
    "+ **spark.executor.memory:** set paling tidak <= dari set pada SPARK_WORKER_MEMORY config pada docker-compose.yml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-machineLearnNB\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contoh Koding Spark Algoritma Naive Bayes (NB) dengan Resilient Distributed Datasets (RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://docs.google.com/uc?export=download&id=1F4oVz0WdWLj6SX5xdvJGiT-BJvZ4wRLZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   280  100   280    0     0   1212      0 --:--:-- --:--:-- --:--:--  1212\n"
     ]
    }
   ],
   "source": [
    "# Collect data\n",
    "# Cara get dataset, misal dari file csv berikut:\n",
    "!curl https://raw.githubusercontent.com/imamcs19/Big-Data-App-based-AI-with-or-without-Web-and-Mobile-Interface-by-RESTful-API-Using-Hadoop-and-Spark/master/1.%20code%20for%20create%20NBMapReduce.jar/NBMapReduce/dataset.csv > dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan dataRDD:\n",
      "[['Data ke-i', 'Fitur 1', 'Fitur 2', 'Fitur 3', 'Kelas (C)'], ['1', 'Urgent', 'Yes', 'Yes', 'Reading'], ['2', 'Urgent', 'No', 'Yes', 'Learn'], ['3', 'Near', 'Yes', 'Yes', 'Reading'], ['4', 'None', 'Yes', 'No', 'Reading'], ['5', 'None', 'No', 'Yes', 'Listening'], ['6', 'None', 'Yes', 'No', 'Reading'], ['7', 'Near', 'No', 'No', 'Learn'], ['8', 'Near', 'No', 'Yes', 'Thinking'], ['9', 'Near', 'Yes', 'Yes', 'Reading'], ['10', 'Urgent', 'No', 'No', 'Learn']]\n",
      "\n",
      "Menampilkan dataDF:\n",
      "+---------+-------+-------+-------+---------+\n",
      "|       _1|     _2|     _3|     _4|       _5|\n",
      "+---------+-------+-------+-------+---------+\n",
      "|Data ke-i|Fitur 1|Fitur 2|Fitur 3|Kelas (C)|\n",
      "|        1| Urgent|    Yes|    Yes|  Reading|\n",
      "|        2| Urgent|     No|    Yes|    Learn|\n",
      "|        3|   Near|    Yes|    Yes|  Reading|\n",
      "|        4|   None|    Yes|     No|  Reading|\n",
      "|        5|   None|     No|    Yes|Listening|\n",
      "|        6|   None|    Yes|     No|  Reading|\n",
      "|        7|   Near|     No|     No|    Learn|\n",
      "|        8|   Near|     No|    Yes| Thinking|\n",
      "|        9|   Near|    Yes|    Yes|  Reading|\n",
      "|       10| Urgent|     No|     No|    Learn|\n",
      "+---------+-------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "# dataRDD = sc.textFile(\"dataset.csv\") \\\n",
    "#     .map(lambda line: line.split(\",\")) \\\n",
    "#     .zipWithIndex().filter(lambda baris: baris[1] > 0) \\\n",
    "#     .map(lambda x:x[0])\n",
    "#     .map(lambda x:(x[0],int(x[1])))\n",
    "\n",
    "dataRDD = sc.textFile(\"dataset.csv\") \\\n",
    "    .map(lambda line: line.split(\",\")) \n",
    "\n",
    "print(\"Menampilkan dataRDD:\")\n",
    "print(dataRDD.collect())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Convert dataRDD ke dataDF\n",
    "dataDF = spark.createDataFrame(dataRDD)\n",
    "print(\"Menampilkan dataDF:\")\n",
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan dataRDD:\n",
      "[['1', 'Urgent', 'Yes', 'Yes', 'Reading'], ['2', 'Urgent', 'No', 'Yes', 'Learn'], ['3', 'Near', 'Yes', 'Yes', 'Reading'], ['4', 'None', 'Yes', 'No', 'Reading'], ['5', 'None', 'No', 'Yes', 'Listening'], ['6', 'None', 'Yes', 'No', 'Reading'], ['7', 'Near', 'No', 'No', 'Learn'], ['8', 'Near', 'No', 'Yes', 'Thinking'], ['9', 'Near', 'Yes', 'Yes', 'Reading'], ['10', 'Urgent', 'No', 'No', 'Learn']]\n",
      "\n",
      "Menampilkan dataDF:\n",
      "+---+------+---+---+---------+\n",
      "| _1|    _2| _3| _4|       _5|\n",
      "+---+------+---+---+---------+\n",
      "|  1|Urgent|Yes|Yes|  Reading|\n",
      "|  2|Urgent| No|Yes|    Learn|\n",
      "|  3|  Near|Yes|Yes|  Reading|\n",
      "|  4|  None|Yes| No|  Reading|\n",
      "|  5|  None| No|Yes|Listening|\n",
      "|  6|  None|Yes| No|  Reading|\n",
      "|  7|  Near| No| No|    Learn|\n",
      "|  8|  Near| No|Yes| Thinking|\n",
      "|  9|  Near|Yes|Yes|  Reading|\n",
      "| 10|Urgent| No| No|    Learn|\n",
      "+---+------+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get content dataRDD\n",
    "# hapus header\n",
    "isi_dataRDD = dataRDD.zipWithIndex().filter(lambda baris:baris[1]>0).map(lambda x:x[0])\n",
    "\n",
    "print(\"Menampilkan dataRDD:\")\n",
    "print(isi_dataRDD.collect())\n",
    "\n",
    "print()\n",
    "\n",
    "dataDF = spark.createDataFrame(isi_dataRDD)\n",
    "print(\"Menampilkan dataDF:\")\n",
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan dataRDD:\n",
      "[['Urgent', 'Yes', 'Yes', 'Reading'], ['Urgent', 'No', 'Yes', 'Learn'], ['Near', 'Yes', 'Yes', 'Reading'], ['None', 'Yes', 'No', 'Reading'], ['None', 'No', 'Yes', 'Listening'], ['None', 'Yes', 'No', 'Reading'], ['Near', 'No', 'No', 'Learn'], ['Near', 'No', 'Yes', 'Thinking'], ['Near', 'Yes', 'Yes', 'Reading'], ['Urgent', 'No', 'No', 'Learn']]\n",
      "\n",
      "Menampilkan dataDF:\n",
      "+----------+----------+----------+---------+\n",
      "|Fitur ke-1|Fitur ke-2|Fitur ke-3|    Kelas|\n",
      "+----------+----------+----------+---------+\n",
      "|    Urgent|       Yes|       Yes|  Reading|\n",
      "|    Urgent|        No|       Yes|    Learn|\n",
      "|      Near|       Yes|       Yes|  Reading|\n",
      "|      None|       Yes|        No|  Reading|\n",
      "|      None|        No|       Yes|Listening|\n",
      "|      None|       Yes|        No|  Reading|\n",
      "|      Near|        No|        No|    Learn|\n",
      "|      Near|        No|       Yes| Thinking|\n",
      "|      Near|       Yes|       Yes|  Reading|\n",
      "|    Urgent|        No|        No|    Learn|\n",
      "+----------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hapus kolom pertama\n",
    "isi_dataRDD2 = isi_dataRDD.map(lambda x:x[1:])\n",
    "dataDF = spark.createDataFrame(isi_dataRDD2).toDF(\"Fitur ke-1\", \"Fitur ke-2\",\"Fitur ke-3\",\"Kelas\")\n",
    "\n",
    "print(\"Menampilkan dataRDD:\")\n",
    "print(isi_dataRDD2.collect())\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Menampilkan dataDF:\")\n",
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "\n",
      "Menampilkan FrekTiapKelasRDD:\n",
      "[('Reading', 5.0), ('Learn', 3.0), ('Listening', 1.0), ('Thinking', 1.0)]\n",
      "\n",
      "Menampilkan FrekTiapKelasDF:\n",
      "+---------+---------------+\n",
      "|    Kelas|Frek Tiap Kelas|\n",
      "+---------+---------------+\n",
      "|  Reading|            5.0|\n",
      "|    Learn|            3.0|\n",
      "|Listening|            1.0|\n",
      "| Thinking|            1.0|\n",
      "+---------+---------------+\n",
      "\n",
      "Menampilkan priorRDD:\n",
      "[('Reading', 0.5), ('Learn', 0.3), ('Listening', 0.1), ('Thinking', 0.1)]\n",
      "\n",
      "Menampilkan priorDF:\n",
      "+---------+----------+\n",
      "|    Kelas|Pel. Prior|\n",
      "+---------+----------+\n",
      "|  Reading|       0.5|\n",
      "|    Learn|       0.3|\n",
      "|Listening|       0.1|\n",
      "| Thinking|       0.1|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# byk_dataTrain = isi_dataRDD2.map(lambda x:1).reduce(lambda x,y:x+y)\n",
    "byk_dataTrain = isi_dataRDD2.count()\n",
    "print(byk_dataTrain)\n",
    "print()\n",
    "\n",
    "#hitung kemunculan tiap kelas\n",
    "FrekTiapKelasRDD = isi_dataRDD2.map(lambda x:(x[3],1.0)).reduceByKey(lambda x,y:x+y)\n",
    "dataDF = spark.createDataFrame(FrekTiapKelasRDD).toDF(\"Kelas\",\"Frek Tiap Kelas\")\n",
    "\n",
    "print(\"Menampilkan FrekTiapKelasRDD:\")\n",
    "print(FrekTiapKelasRDD.collect())\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Menampilkan FrekTiapKelasDF:\")\n",
    "dataDF.show()\n",
    "\n",
    "# hitung peluang prior atau peluang kelas\n",
    "priorRDD = isi_dataRDD2.map(lambda x:(x[3],1.0)).reduceByKey(lambda x,y:x+y).map(lambda x:(x[0],x[1]/byk_dataTrain))\n",
    "\n",
    "dataDF = spark.createDataFrame(priorRDD).toDF(\"Kelas\",\"Pel. Prior\")\n",
    "\n",
    "print(\"Menampilkan priorRDD:\")\n",
    "print(priorRDD.collect())\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Menampilkan priorDF:\")\n",
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan dataTestRDD:\n",
      "[('1', 'Urgent', 'Yes', 'Yes', 'Reading'), ('2', 'Urgent', 'No', 'No', 'Learn')]\n",
      "\n",
      "Menampilkan dataTestDF:\n",
      "+---+------+---+---+-------+\n",
      "| _1|    _2| _3| _4|     _5|\n",
      "+---+------+---+---+-------+\n",
      "|  1|Urgent|Yes|Yes|Reading|\n",
      "|  2|Urgent| No| No|  Learn|\n",
      "+---+------+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Misal, sebagai contoh dataTest-nya berikut set atau load dari file\n",
    "dataTestRDD = sc.parallelize([('1', 'Urgent', 'Yes', 'Yes', 'Reading'),('2', 'Urgent', 'No', 'No', 'Learn')])\n",
    "# dataTestRDD = sc.parallelize([(\"Urgent\"),(\"Yes\"),(\"Yes\")])\n",
    "print(\"Menampilkan dataTestRDD:\")\n",
    "print(dataTestRDD.collect())\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Menampilkan dataTestDF:\")\n",
    "dataDF = spark.createDataFrame(dataTestRDD)\n",
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kode Utama Algoritma Naive Bayes dengan Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Urgent', 'Yes', 'Yes')]\n",
      "\n",
      "Fitur ke-1: Urgent\n",
      "\n",
      "Menampilkan getFiturNkelasDF:\n",
      "+-------+---+\n",
      "|     _1| _2|\n",
      "+-------+---+\n",
      "|  Learn|  2|\n",
      "|Reading|  1|\n",
      "+-------+---+\n",
      "\n",
      "init P_posterior ( Reading | data Test ): 0.2\n",
      "\n",
      "init P_posterior ( Learn | data Test ): 0.6666666666666666\n",
      "\n",
      "init P_posterior ( Listening | data Test ): 1e-07\n",
      "\n",
      "init P_posterior ( Thinking | data Test ): 1e-07\n",
      "\n",
      "\n",
      "Fitur ke-2: Yes\n",
      "\n",
      "Menampilkan getFiturNkelasDF:\n",
      "+-------+---+\n",
      "|     _1| _2|\n",
      "+-------+---+\n",
      "|Reading|  5|\n",
      "+-------+---+\n",
      "\n",
      "init P_posterior ( Reading | data Test ): 0.2\n",
      "\n",
      "init P_posterior ( Learn | data Test ): 2.222222222222222e-08\n",
      "\n",
      "init P_posterior ( Listening | data Test ): 9.999999999999998e-15\n",
      "\n",
      "init P_posterior ( Thinking | data Test ): 9.999999999999998e-15\n",
      "\n",
      "\n",
      "Fitur ke-3: Yes\n",
      "\n",
      "Menampilkan getFiturNkelasDF:\n",
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|  Reading|  3|\n",
      "|    Learn|  1|\n",
      "|Listening|  1|\n",
      "| Thinking|  1|\n",
      "+---------+---+\n",
      "\n",
      "init P_posterior ( Reading | data Test ): 0.12\n",
      ">>>>> final P_posterior ( Reading | data Test ): 0.06\n",
      "\n",
      "init P_posterior ( Learn | data Test ): 7.407407407407406e-09\n",
      ">>>>> final P_posterior ( Learn | data Test ): 2.2222222222222217e-09\n",
      "\n",
      "init P_posterior ( Listening | data Test ): 9.999999999999998e-15\n",
      ">>>>> final P_posterior ( Listening | data Test ): 9.999999999999999e-16\n",
      "\n",
      "init P_posterior ( Thinking | data Test ): 9.999999999999998e-15\n",
      ">>>>> final P_posterior ( Thinking | data Test ): 9.999999999999999e-16\n",
      "\n",
      "\n",
      "Menampilkan hasil kelas dataTest\n",
      "final P_posterior ( Reading | data Test ): 0.06\n",
      "final P_posterior ( Learn | data Test ): 2.2222222222222217e-09\n",
      "final P_posterior ( Listening | data Test ): 9.999999999999999e-16\n",
      "final P_posterior ( Thinking | data Test ): 9.999999999999999e-16\n",
      "\n",
      "Jadi data test [('Urgent', 'Yes', 'Yes')]  masuk pada kelas :\n",
      "Reading\n"
     ]
    }
   ],
   "source": [
    "byk_fitur = 3\n",
    "fitur = []\n",
    "kelas = ['Reading','Learn', 'Listening', 'Thinking']\n",
    "byk_kelas = len(kelas)\n",
    "\n",
    "P_posterior = [1.,1.,1.,1.]\n",
    "\n",
    "# set Data Testing\n",
    "pilih_dataTest_by_Idx = 0\n",
    "onedataTestRDD = dataTestRDD.zipWithIndex().filter(lambda baris:baris[1]==pilih_dataTest_by_Idx).map(lambda x:x[0][1:4])\n",
    "print(onedataTestRDD.collect())\n",
    "\n",
    "print()\n",
    "\n",
    "for i in range(0,byk_fitur):\n",
    "    # get fitur dari rdd dari data Test\n",
    "    fitur.append(onedataTestRDD.map(lambda x:x[i]).collect()[0])\n",
    "    print(\"Fitur ke-\"+str(i+1)+\": \"+ fitur[i])\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    #inisialisasi frekuensi dari kombinasi tiap fitur data teset dengan label kelas yg ada\n",
    "    # untuk persiapan menghitung peluang likelihood atau P(fitur | kelas)\n",
    "    getFiturNkelasRDD = isi_dataRDD2.filter(lambda x:(x[i]==fitur[i])).map(lambda x:(x[3],1)). \\\n",
    "                reduceByKey(lambda x,y:x+y)\n",
    "    #print(getFiturNkelasRDD.collect())\n",
    "    \n",
    "    print(\"Menampilkan getFiturNkelasDF:\")\n",
    "    getFiturNkelasDF = spark.createDataFrame(getFiturNkelasRDD)\n",
    "    getFiturNkelasDF.show()\n",
    "    \n",
    "    \n",
    "    # hitung likelihood utk tiap kelas\n",
    "    # P_likelihood = P(f1 | Reading)*P(f2 | Reading)*P(f3 | Reading)\n",
    "    # untuk menghitung peluang Posterior\n",
    "\n",
    "    for j in range(0,byk_kelas):\n",
    "        # Get Peluang Likelihood\n",
    "        if getFiturNkelasRDD.filter(lambda x:x[0]==kelas[j]).map(lambda x:x[1]).isEmpty() == True:\n",
    "            # untuk smoothing\n",
    "            cur_P_Likelihood = 0.0000001\n",
    "        else:\n",
    "            cur_P_Likelihood = getFiturNkelasRDD.filter(lambda x:x[0]==kelas[j]).map(lambda x:x[1]).collect()[0]\n",
    "        \n",
    "        P_posterior[j] *= cur_P_Likelihood/ \\\n",
    "                          FrekTiapKelasRDD.filter(lambda x:x[0]==kelas[j]).map(lambda x:x[1]).collect()[0]\n",
    "\n",
    "        #print(j)\n",
    "        #print(\"init P_posterior[\",j,\"] = \", P_posterior[j])\n",
    "        print(\"init P_posterior (\",kelas[j],\"| data Test ):\", P_posterior[j])\n",
    "        \n",
    "        # Mengalikan dengan Prior, setelah index fitur terakhir\n",
    "        if(i==byk_fitur-1):\n",
    "            P_posterior[j] *= priorRDD.filter(lambda x:x[0]==kelas[j]).map(lambda x:x[1]).collect()[0]\n",
    "            print(\">>>>> final P_posterior (\",kelas[j],\"| data Test ):\", P_posterior[j])\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    print()\n",
    "    \n",
    "P_posteriorRDD = sc.parallelize(P_posterior)\n",
    "    \n",
    "# Menampilkan hasil kelas dataTest\n",
    "print('Menampilkan hasil kelas dataTest')\n",
    "for j in range(0,byk_kelas):\n",
    "    print(\"final P_posterior (\",kelas[j],\"| data Test ):\", P_posterior[j])\n",
    "\n",
    "print()\n",
    "\n",
    "# Get ArgMax atau mengambil index dari nilai tertinggi, di mana index dimulai dari nol\n",
    "# contoh ArgMax(23,9,63,1) = 2, vs Max(23,9,63,1) = 63\n",
    "ArgMax = P_posteriorRDD.zipWithIndex().sortBy(lambda x:x[0], ascending=False).map(lambda x:x[1]).collect()[0]\n",
    "print(\"Jadi data test\", onedataTestRDD.collect(), \" masuk pada kelas :\")\n",
    "print(kelas[ArgMax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref:\n",
    "# [1] Cholissodin, I., Riyandani, E., 2016, Analisis Big Data, Fakultas Ilmu Komputer, Universitas Brawijaya, Malang.\n",
    "# [2] Cholissodin, I. and Supianto, A. A. 2019. Enhancement Full Open Source Hadoop Distribution Universal Big Data Up Projects (UBig) From Education To Enterprise. 2019 International Conference on Sustainable Information Engineering and Technology (SIET), Lombok, Indonesia, 2019, pp. 90-93, doi: 10.1109/SIET48054.2019.8986040.\n",
    "# [3] Miao, K., Li, J., Hong, W. & Chen, M. 2020. A Microservice-Based Big Data Analysis Platform for Online Educational Applications. Annual Review of Anthropology, 2020, [ \"6929750\" ]. Available from: https://doi.org/10.1146/annurev.anthro.33.070203.144008\n",
    "# [4] Roy, S., et al. 2017. IoT, big data science & analytics, cloud computing and mobile app based hybrid system for smart agriculture. 2017 8th Annual Industrial Automation and Electromechanical Engineering Conference (IEMECON), Bangkok, 2017, pp. 303-304, doi: 10.1109/IEMECON.2017.8079610.\n",
    "# [5] Dabek, F. 2016. Leveraging Big Data to Provide a Web Service That Provides the Likelihood of Developing Psychological Conditions after a Concussion. 2016 IEEE International Conference on Mobile Services (MS), San Francisco, CA, 2016, pp. 160-165, doi: 10.1109/MobServ.2016.32.\n",
    "# [6] Andy A. 2016. Cloud Computing Part 5: SaaS (Software as a Service). Available at: https://andypi.co.uk/2016/05/23/cloud-computing-part-5-saas-software-as-a-service/ (Accessed on July 30, 2020).\n",
    "# [7] Putra, N. A., Putri, A. T., Prabowo, D. A., Surtiningsih, L., Arniantya, R., Cholissodin, I. 2017. Klasifikasi Sepeda Motor Berdasarkan Karakteristik Konsumen Dengan Metode K-Nearest Neighbour Pada Big Data Menggunakan Hadoop Single Node Cluster. Jurnal Teknologi Informasi dan Ilmu Komputer (JTIIK) FILKOM UB Vol. 4 No. 2, 81-86.\n",
    "# [8] Naveen, N. 2020. Hadoop MapReduce â Key Features & Highlights. Available at: https://intellipaat.com/blog/tutorial/big-data-and-hadoop-tutorial/hadoop-mapreduce-key-features-highlights/ (Accessed on March 8, 2020).\n",
    "# [9] stackchief. 2017. MapReduce Quick Explanation. Available at: https://www.stackchief.com/blog/MapReduce%20Quick%20Explanation (Accessed on March 8, 2020).\n",
    "# [10] .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/thumbs-up.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='another_cell'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
